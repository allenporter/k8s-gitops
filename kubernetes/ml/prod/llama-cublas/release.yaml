---
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: llama-cublas
spec:
  releaseName: llama-cublas
  chart:
    spec:
      chart: app-template
      interval: 30m
      sourceRef:
        kind: HelmRepository
        name: bjw-s-helm-charts
        namespace: flux-system
      version: 2.4.0
  interval: 5m
  install:
    remediation:
      retries: 3
  test:
    # Fix problem where helm fails to uninstall
    enable: false
  values:
    defaultPodOptions:
      runtimeClassName: nvidia
    controllers:
      main:
        strategy: Recreate
        initContainers:
          download-mistral-v01-files:
            image:
              repository: ghcr.io/allenporter/llama-cpp-server-cuda
              tag: v2.21.1
              pullPolicy: IfNotPresent
            command: ["huggingface-cli"]
            args:
            - download
            - TheBloke/Mistral-7B-Instruct-v0.1-GGUF
            - --exclude
            - '*gguf'
            - --local-dir=/data/models/Mistral-7B-Instruct-v0.1
          download-mistral-v01-model:
            image:
              repository: ghcr.io/allenporter/llama-cpp-server-cuda
              tag: v2.21.1
              pullPolicy: IfNotPresent
            command: ["huggingface-cli"]
            args:
            - download
            - TheBloke/Mistral-7B-Instruct-v0.1-GGUF
            - mistral-7b-instruct-v0.1.Q4_K_M.gguf
            - --local-dir=/data/models/Mistral-7B-Instruct-v0.1

          download-functionary-7b-v1-files:
            image:
              repository: ghcr.io/allenporter/llama-cpp-server-cuda
              tag: v2.21.1
              pullPolicy: IfNotPresent
            command: ["huggingface-cli"]
            args:
            - download
            - meetkai/functionary-7b-v1.4-GGUF
            - --exclude
            - '*gguf'
            - --local-dir=/data/models/functionary-7b-v1.4
          download-functionary-7b-v1-model:
            image:
              repository: ghcr.io/allenporter/llama-cpp-server-cuda
              tag: v2.21.1
              pullPolicy: IfNotPresent
            command: ["huggingface-cli"]
            args:
            - download
            - meetkai/functionary-7b-v1.4-GGUF
            - functionary-7b-v1.4.q4_0.gguf
            - --local-dir=/data/models/functionary-7b-v1.4
          download-functionary-7b-v2-1-files:
            image:
              repository: ghcr.io/allenporter/llama-cpp-server-cuda
              tag: v2.21.1
              pullPolicy: IfNotPresent
            command: ["huggingface-cli"]
            args:
            - download
            - meetkai/functionary-7b-v2.1-GGUF
            - --exclude
            - '*gguf'
            - --local-dir=/data/models/functionary-7b-v2.1
          download-functionary-7b-v2-1-model:
            image:
              repository: ghcr.io/allenporter/llama-cpp-server-cuda
              tag: v2.21.1
              pullPolicy: IfNotPresent
            command: ["huggingface-cli"]
            args:
            - download
            - meetkai/functionary-7b-v2.1-GGUF
            - functionary-7b-v2.1.q4_0.gguf
            - --local-dir=/data/models/functionary-7b-v2.1
        containers:
          main:
            image:
              repository: ghcr.io/allenporter/llama-cpp-server-cuda
              tag: v2.21.1
              pullPolicy: IfNotPresent
            env:
            - name: MODEL_DIR
              value: /data/models
            - name: CONFIG_FILE
              value: /config/model-config.json
            resources:
              limits:
                nvidia.com/gpu: 1

    service:
      main:
        ports:
          http:
            port: 8000

    ingress:
      main:
        enabled: true
        annotations:
          cert-manager.io/cluster-issuer: letsencrypt
        hosts:
        - host: llama-cublas.${name_service_dns_domain}
          paths:
          - path: /
            service:
              name: main
              port: http
        tls:
        - secretName: llama-cublas-tls
          hosts:
          - llama-cublas.${name_service_dns_domain}

    persistence:
      data:
        enabled: true
        storageClassName: "local-hostpath"
        type: persistentVolumeClaim
        accessMode: ReadWriteOnce
        size: 40Gi
        globalMounts:
        - path: /data
          readOnly: false
        retain: true

      config:
        enabled: true
        type: configMap
        name: cublas-model-config
        globalMounts:
        - path: /config/model-config.json
          subPath: model-config.json
          readOnly: true

      cache:
        enabled: true
        storageClassName: "local-hostpath"
        type: persistentVolumeClaim
        accessMode: ReadWriteOnce
        size: 40Gi
        globalMounts:
        - path: /root/.cache/
          readOnly: false
        retain: true
