{
  "host": "0.0.0.0",
  "port": 8080,
  "models": [
    {
      "model": "/data/models/mistral-7b-instruct-v0.1.Q4_K_M.gguf",
      "model_alias": "mistral-7b-instruct-v1",
      "chat_format": "llama-2",
      "n_gpu_layers": 35,
      "offload_kqv": true,
      "n_ctx": 4096,
      "use_mlock": false
    },
    {
      "model": "/data/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf",
      "model_alias": "mistral-7b-instruct-v2",
      "chat_format": "llama-2",
      "n_gpu_layers": 35,
      "offload_kqv": true,
      "n_ctx": 4096,
      "use_mlock": false
    },
    {
      "model": "/data/models/Mistral-7B-Instruct-v0.3.Q3_K_M.gguf",
      "model_alias": "mistral-7b-instruct-v3",
      "chat_format": "llama-2",
      "n_gpu_layers": 35,
      "offload_kqv": true,
      "n_ctx": 4096,
      "use_mlock": false
    },
    {
      "model": "/data/models/Meta-Llama-3-8B-Instruct.Q4_K_M.gguf",
      "model_alias": "llama-3-8b-instruct",
      "chat_format": "llama-3",
      "n_gpu_layers": 35,
      "offload_kqv": true,
      "n_ctx": 8192,
      "use_mlock": false
    },
    {
      "model": "/data/models/Hermes-2-Pro-Llama-3-8B-Q4_K_M.gguf",
      "model_alias": "hermes-2-pro-llama-3",
      "chat_format": "chatml-function-calling",
      "n_gpu_layers": 35,
      "offload_kqv": true,
      "n_ctx": 8192,
      "use_mlock": false
    }
  ]
}
