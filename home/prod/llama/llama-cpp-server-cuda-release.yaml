---
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: llama-cpp-server-cuda
  namespace: llama
spec:
  releaseName: llama-cpp-server-cuda
  chart:
    spec:
      chart: app-template
      interval: 30m
      sourceRef:
        kind: HelmRepository
        name: bjw-s-helm-charts
        namespace: flux-system
  interval: 5m
  install:
    remediation:
      retries: 3
  test:
    # Fix problem where helm fails to uninstall
    enable: false
  values:
    controllers:
      main:
        strategy: Recreate
        initContainers:
          download-model:
            image:
              repository: busybox
              pullPolicy: IfNotPresent
            command: ["/bin/sh", "-c"]
            args:
            - |
              MODEL_DIR=/data/models
              FORCE_DOWNLOAD=false
              URLS="https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf"

              mkdir -p "$MODEL_DIR"

              # Split urls on commas
              echo "$URLS" | awk -F, '{for (i=1; i<=NF; i++) print $i}' | while read -r line; do
                  url=$(echo "$line" | awk '{print $1}')

                  if [ -n "$url" ]; then
                      filename=$(basename "$url" .bin)

                      if [ "$FORCE_DOWNLOAD" = false ] && [ -f "$MODEL_DIR/$filename" ]; then
                          echo "File $filename already exists. Skipping download."
                          continue
                      fi
                      rm -f "$MODEL_DIR/$filename"

                      echo "Downloading $filename"
                      wget "$url" -O "$MODEL_DIR/$filename"

                      if [ "$?" -ne 0 ]; then
                          echo "Download failed."
                      else
                          echo "Download completed."
                      fi
                  fi
              done

        containers:
          main:
            image:
              repository: ghcr.io/allenporter/llama-cpp-server-cuda
              pullPolicy: IfNotPresent
              tag: v0.1.0

    service:
      main:
        ports:
          http:
            port: 8000

    ingress:
      main:
        enabled: true
        annotations:
          cert-manager.io/cluster-issuer: letsencrypt
          haproxy.org/forwarded-for: "true"
          hajimari.io/icon: transform
          hajimari.io/appName: LLama.cpp Cuda Server
          external-dns.alpha.kubernetes.io/hostname: llama-server-cuda.${DOMAIN}.
          external-dns.alpha.kubernetes.io/target: prx02.${DOMAIN}.
        hosts:
        - host: llama-server-cuda.${DOMAIN}
          paths:
          - path: /
            service:
              name: main
              port: http
        tls:
        - secretName: llama-cpp-server-tls
          hosts:
          - llama-server-cuda.${DOMAIN}
        resources:
          limits:
            nvidia.com/gpu: 1

    persistence:
      data:
        enabled: true
        type: persistentVolumeClaim
        accessMode: ReadWriteOnce
        size: 40Gi
